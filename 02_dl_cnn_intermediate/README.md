####
## ğŸ“Œ Convolutional Neural Networks II
####
#### â–º [01_dl_keras_framework_sequential_api_v1_220629]
- Fashion MNIST datasetì„ ì´ìš©í•´, 10ê°œì˜ ì˜ë¥˜ classì— ì†í•  í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ìƒì„±
- train / test xì˜ í”½ì…€ê°’ ë²”ìœ„ë¥¼ 0-1 ì‚¬ì´ ê°’ìœ¼ë¡œ scalingí•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ ë³€í™” í™•ì¸
- labelì˜ one-hot encoding ì—¬ë¶€ì— ë”°ë¼, categorical crossentropy loss function ì ìš© ì‹¤ìŠµ
- label ê°’ì´ 10ê°œì˜ class ì¤‘ì— ì†í•  í™•ë¥ ì„ ë‚˜íƒ€ëƒ„ì— ë”°ë¼, ì´í•©ì´ 1ì¸ softmax activation function ì ìš© ì‹¤ìŠµ
- í•œ ì¥ì˜ 2ì°¨ì› ë°°ì—´ ì´ë¯¸ì§€ì˜ ê²½ìš°, expand_dims()ë¥¼ í†µí•´ 3ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ í›„ fit / predict() ì§„í–‰ í•„ìš” í™•ì¸
####
#### â–º [02_dl_keras_framework_functional_api_v2_220701]
- dense layerë¥¼ ê¸°ë°˜ìœ¼ë¡œ functional APIë¥¼ í™œìš©í•œ ëª¨ë¸ ìƒì„± ì‹¤ìŠµ
- model.summary() ì¶œë ¥ ì‹œ, biasê°€ í¬í•¨ëœ parameter ìˆ˜ì˜ ì´í•´ì™€ ê³„ì‚° ì‹¤ìŠµ
####  
#### â–º [03_dl_cnn_baseline_model_v3_220704]
- CIFAR10 datasetì„ ì´ìš©í•´, 10ê°œì˜ object classì— ì†í•  í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ìƒì„±
- conv ì—°ì‚°ì„ ì—°ë‹¬ì•„ ì ìš©í•œ í›„, max poolingì„ ë°°ì¹˜í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ model ìƒì„±
- conv ì ìš© ì‹œ, batch sizeë¥¼ í¬í•¨í•œ 4ì°¨ì› ë°°ì—´ì˜ ì´ë¯¸ì§€ arrayë§Œ ì…ë ¥ ê°€ëŠ¥í•œ ì  í™•ì¸
- overfittingì˜ ì´í•´ì™€ batch ë‹¨ìœ„ì˜ ëœë¤ ë¹„ìœ¨ ë…¸ë“œë¥¼ dropí•˜ëŠ” dropoutì˜ ì´í•´ì™€ ì ìš© ì‹¤ìŠµ
- ResNet ì´í›„ feature ì†ì‹¤ ì´ìŠˆë¡œ poolingì´ ì•„ë‹Œ strideë¡œ feature mapì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²½í–¥ í™•ì¸
####
#### â–º [04_dl_cnn_batch_normalization_v4_220704]
- batch normalization ì ìš© ë° ì˜ˆì¸¡ ì„±ëŠ¥ ì‹¤ìŠµ
- inputì´ layerë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ ë¶„í¬ê°€ ë³€ê²½ë˜ëŠ” í˜„ìƒ(internal covariate shift)ì„ ë°©ì§€í•˜ê¸° ìœ„í•¨
- conv > batch normalization > activation ìˆœì„œ ë°°ì¹˜
- âœ“ ì‹¤ìŠµ ê²°ê³¼, ê¸°ì¡´ 2-3 ì‹¤ìŠµì— ë¹„í•´ test accuracyê°€ ì•½ 0.8030ì—ì„œ 0.8287ë¡œ í–¥ìƒ
####
#### â–º [05_dl_cnn_batch_size_v5_220705]
- batch sizeì— ë”°ë¥¸ ì˜ˆì¸¡ ì„±ëŠ¥ ì‹¤ìŠµ
- batch sizeê°€ ì‘ì€ ê²½ìš°, ìƒëŒ€ì ìœ¼ë¡œ ìì£¼ weightë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì •í™•í•œ ìµœì í™”ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë…¼ë¬¸ ë‚´ìš©ì— ê¸°ë°˜
- âœ“ ì‹¤ìŠµ ê²°ê³¼, batch sizeê°€ ì‘ì€ ê²½ìš° ë¹„êµì  ì˜ˆì¸¡ ì„±ëŠ¥ì´ í–¥ìƒë¨ê³¼ ë™ì‹œì— ë³€ë™ ì•ˆì •ì„±ì´ ë†’ìŒì„ í™•ì¸
####
#### â–º [06_dl_cnn_callback_v6_220705]
- callback function ì ìš© ì‹¤ìŠµ
- í•™ìŠµ interation ì‹œ, ë“±ë¡í•œ callback APIì— ë”°ë¼ íŠ¹ì • ì´ë²¤íŠ¸ ë°œìƒ ì‹œ learning rate ë“±ì˜ ë™ì  ì‘ì—… ìˆ˜í–‰
- ë³¸ ì‹¤ìŠµì—ì„œ ìƒì„± ë° ë“±ë¡í•œ callback API ì¡°ê±´ì€ í•˜ê¸°ì™€ ê°™ìŒ
- ModelCheckpoint : val_lossê°€ ê°€ì¥ ë‚®ì•„ì§€ëŠ” ì‹œì ì˜ weightë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ë³„ë„ ì €ì¥
- ReduceLROnPlateau : patience ê¸°ì¤€, val_lossê°€ ë‚®ì•„ì§€ì§€ ì•Šì„ ê²½ìš° learning rate ê°±ì‹ 
- EarlyStopping : patience ê¸°ì¤€, val_lossê°€ ë‚®ì•„ì§€ì§€ ì•Šì„ ê²½ìš° í•™ìŠµ ì¤‘ë‹¨
####
#### â–º [07_dl_cnn_global_average_pooling_v7_220706]
- global average pooling ì ìš© ì‹¤ìŠµ
- feature mapì˜ íŠ¹ì • ì˜ì—­ì„ sub samplingì´ ì•„ë‹Œ ì±„ë„ë³„ í‰ê·  ê°’ì„ ì¶”ì¶œí•¨
- feature map ì±„ë„ ìˆ˜ê°€ ë§ì„ ê²½ìš° global average poolingì„ ì ìš©í•˜ë‚˜, ì ì„ ê²½ìš° flatten ì ìš©ì´ ìœ ë¦¬í•¨
- 3ì°¨ì› feature mapê³¼ 1ì°¨ì› classifier ì—°ê²° ì‹œ, nodeì™€ parameter ìˆ˜ë¥¼ ê°ì†Œì‹œì¼œ overfitting ë°©ì§€
- node ìˆ˜ : flatten(8,192ê°œ) / global average pooling(512ê°œ)
- parameter ìˆ˜ : flatten(2,457,900ê°œ) / global average pooling(153,900ê°œ)
####
#### â–º [08_dl_cnn_data_augmentation_v8_220707]
- data augmentation ì ìš© ë° ì˜ˆì¸¡ ì„±ëŠ¥ ì‹¤ìŠµ
- dataë¥¼ ëŠ˜ë¦¬ê¸° ì–´ë ¤ìš´ ì´ë¯¸ì§€ì˜ ê²½ìš°, augumentationì„ í†µí•´ ë‹¤ì–‘í•œ í•™ìŠµ data ì–‘ì„ ëŠ˜ë ¤ overfitting ë°©ì§€
- processing pipelineì´ ì œê³µë˜ë‚˜ ëœë¤ ì ìš©ìœ¼ë¡œ ë³€í™˜ í™•ë¥ ì„ ì„¤ì •í•  ìˆ˜ ì—†ìŒ
- validation / testê°€ ì•„ë‹Œ train ë°ì´í„°ì— í•œí•´ ì ìš©í•˜ëŠ” ì  í™•ì¸
- âœ“ ì‹¤ìŠµ ê²°ê³¼, ê¸°ì¡´ 2-7 ì‹¤ìŠµì— ë¹„í•´ test accuracyê°€ ì•½ 0.8619ì—ì„œ 0.8949ë¡œ í–¥ìƒ
####
#### â–º [09/10_cnn_transfer_learning_vgg16/xception_v9/10 220707/220708]
- Imagenet datasetìœ¼ë¡œ í›ˆë ¨ëœ pretrained modelì„ í™œìš©í•´ transfer learning ì ìš© ë° ì˜ˆì¸¡ ì„±ëŠ¥ ì‹¤ìŠµ
- include_top false ê°œë… ì´í•´ì™€ customí•œ classifier ì„¤ì • ë°©ë²• ì‹¤ìŠµ
- feature extractor(CNN, activation, pooling) + classifier(fully connected layer, output layer)
- input sizeë¥¼ 32x32ì—ì„œ 64x64ë¡œ ë³€ê²½í•˜ëŠ” ë°©ë²• ì‹¤ìŠµ
- CNN ì‹¤ìŠµì„ í†µí•´ ìŠµë“í•œ ë°©ë²•ì„ ì¼ê´„ í•¨ìˆ˜í™”í•˜ì—¬ ì •ë¦¬í•˜ëŠ” ë°©ë²• ì‹¤ìŠµ
- âœ“ ì‹¤ìŠµ ê²°ê³¼, Xception modelì„ í†µí•œ transfer learning ì§„í–‰ ì‹œ, 0.9308ê¹Œì§€ test accuracy í–¥ìƒ
##  
#### â–º [11_dl_cnn_intermediate_comprehensive_practice_220715]
- stanford dogs datasetì„ ì´ìš©í•´, 120ì—¬ì¢…ì˜ í’ˆì¢…ì„ êµ¬ë¶„í•˜ëŠ” ëª¨ë¸ ìƒì„± ì‹¤ìŠµ
####
